{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BTH3_RS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ10QYTclmg5"
      },
      "source": [
        "### Huỳnh Nhật Hào - 18520714"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfmlPyRYlmhE",
        "outputId": "6efd7d3c-6dce-47a6-b626-059685c051a5"
      },
      "source": [
        "\n",
        "from typing import Union, Optional, Dict, List, Text, Type\n",
        "import string\n",
        "import collections\n",
        "import logging \n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.metrics import pairwise\n",
        "from sklearn import decomposition\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/azureuser/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/azureuser/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS7sHZa90Zmv"
      },
      "source": [
        "handler = logging.StreamHandler()\n",
        "formmater = logging.Formatter(r'%(asctime)s - %(message)s')\n",
        "handler.setFormatter(formmater)\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.addHandler(handler)\n",
        "logger.setLevel(logging.INFO)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab_NuyXDnRpp"
      },
      "source": [
        "class ContentBasedRS:\n",
        "    \"\"\"Content-based Recommender system algorithm\"\"\"\n",
        "\n",
        "    def __init__(self, data: Dict[str, Dict[str, str]],\n",
        "                vocab: Dict[str, int], logger: logging.Logger,\n",
        "                num_features: int = 300 ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data is a dictionary contains id, movie title and review\n",
        "                example: data = {'1': {'name': 'some name', 'overview': 'some overview'}}\n",
        "                \n",
        "            vocab: a dict of vocabularis, where keys are terms and values are\n",
        "                indices in the feature matrix\n",
        "            num_features: number of features after reduction using truncatedSVD\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "        self.logger = logger\n",
        "        self.process_data()\n",
        "        self.movies_name = {value['name']: id for id, value in self.data.items()}\n",
        "\n",
        "        # tf-idf features of all overviews\n",
        "        self.content_features = None\n",
        "\n",
        "        # dimension reduced features of tf-idf features\n",
        "        self.reduced_features = None\n",
        "\n",
        "        self.vectorizer = text.TfidfVectorizer(vocabulary = self.vocab,)\n",
        "        self.num_features = num_features\n",
        "        self.svd = decomposition.TruncatedSVD(n_components= num_features,\n",
        "                                              random_state = 42)\n",
        "\n",
        "    def learn_features(self, ) -> None:\n",
        "        \"\"\"Learn tf-idf features and vectorize all overviews and reduce \n",
        "        feature's dimension using PCA and save them into self.content_features\"\"\"\n",
        "\n",
        "        self.logger.info('Learning tf-idf features...')\n",
        "        all_overviews = [x['overview'] for _, x in self.data.items()]\n",
        "\n",
        "        self.content_features = self.vectorizer.fit_transform(all_overviews)\n",
        "        \n",
        "        self.logger.info('Reducing features dimension to %d'%self.num_features)\n",
        "\n",
        "        self.reduced_features = self.svd.fit_transform(self.content_features)\n",
        "\n",
        "        self.logger.info('Creating linear kernel matrix...')\n",
        "        self.cosine_matrix = pairwise.linear_kernel(self.reduced_features.reshape(len(all_overviews), -1))\n",
        "\n",
        "        assert self.cosine_matrix.shape == (len(self.data), len(self.data))\n",
        "        self.logger.info('Done.')\n",
        "\n",
        "    def process_data(self) -> None:\n",
        "        \"\"\"replace all movies indices with new indices from 0 to len(data)\n",
        "        and preprocess all overviews\"\"\"\n",
        "\n",
        "        self.logger.info('Processing data...')\n",
        "        i = 0\n",
        "        new_data = collections.OrderedDict()\n",
        "        for _, value in self.data.items():\n",
        "            value['overview'] = self.preprocess_text(value['overview'])\n",
        "            new_data[i] = value\n",
        "            i+= 1\n",
        "        self.logger.info('Done processing data.')\n",
        "        self.data = new_data\n",
        "\n",
        "    def preprocess_text(self, texts: str) -> str:\n",
        "        \"\"\"Preprocess one review with all basic steps, also replace out-of-vocab\n",
        "        word with unknown token\n",
        "        \"\"\"\n",
        "        texts = texts.lower()\n",
        "        # remove puntuations\n",
        "        texts = texts.translate(str.maketrans('', '', string.punctuation))\n",
        "        \n",
        "        texts = texts.split()\n",
        "        # remove stopwords\n",
        "        texts = [x for x in texts if x not in stop_words]\n",
        "        # lemmatize words\n",
        "        lemma = nltk.wordnet.WordNetLemmatizer()\n",
        "        texts = [lemma.lemmatize(x) for x in texts]\n",
        "        # replace unknown word with 'unknown'\n",
        "\n",
        "        texts = [x if x in list(self.vocab.keys()) else 'unknown' for x in texts ]\n",
        "        return ' '.join(texts)\n",
        "\n",
        "    def recommend(self, watched: List[str], num_recommend: int) -> List[str]:\n",
        "        \"\"\"Recommend num_recommend movies to a user who has watched movies in \n",
        "        watched. \n",
        "        \"\"\"\n",
        "        names = []\n",
        "        chosen_movies = []\n",
        "        for movie in watched:\n",
        "            if movie in self.movies_name.keys():\n",
        "                names.append(movie)\n",
        "            else:\n",
        "                logger.warning('movie %s not founded'%movie)\n",
        "\n",
        "        if names:\n",
        "            movie_id = self.movies_name[names[0]]\n",
        "            \n",
        "            highest_cosine = np.argsort(self.cosine_matrix[movie_id, :])[-(num_recommend + 1):]\n",
        "    \n",
        "            chosen_movies = {self.data[x]['name']: self.cosine_matrix[movie_id, x] for x in reversed(highest_cosine) }\n",
        "\n",
        "        return chosen_movies\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ayf5gPReyAb"
      },
      "source": [
        "def make_vocab(texts, num_vocab):\n",
        "    \"\"\"Create a vocab mapping {term: index}}\"\"\"\n",
        "    texts = texts.lower()\n",
        "    # remove puntuations\n",
        "    texts = texts.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "    texts = texts.split()\n",
        "    # remove stopwords\n",
        "    texts = [x for x in texts if x not in stop_words]\n",
        "    # lemmatize words\n",
        "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
        "    texts = [lemma.lemmatize(x) for x in texts]\n",
        "    # takes num_vocab most common words\n",
        "    counter = collections.Counter(texts)\n",
        "    selected_words = counter.most_common(num_vocab)\n",
        "    selected_words = [x[0] for x in selected_words]\n",
        "    \n",
        "    vocab = {w: i+1 for i, w in enumerate(selected_words)}\n",
        "    vocab['_unknown_'] = 0\n",
        "    \n",
        "    return vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R1bQ2bGpFKQ"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/movies_metadata.csv.zip', 'r') as zf:\n",
        "    zf.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_1s6nMMexW0",
        "outputId": "f1287192-f185-4acd-8a3d-748c850013a5"
      },
      "source": [
        "df = pd.read_csv('movies_metadata.csv')\n",
        "\n",
        "# filter out duplicate id movies\n",
        "df = df.fillna('')\n",
        "df = df.drop_duplicates('id')\n",
        "\n",
        "# a dictionary {id: int: {name: str, overview: str}}\n",
        "overviews = {row.id: {'name': row['original_title'], 'overview': row['overview']} for i, row in df.iterrows()}\n",
        "# create one text file\n",
        "texts = [x['overview'] for _, x in overviews.items()]\n",
        "texts = ' '.join(texts)\n",
        "\n",
        "test_data = {key: value for i, (key, value) in enumerate(overviews.items()) if i< 1000 }\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMjPZDmeezlD"
      },
      "source": [
        "vocab = make_vocab(texts, 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4UZQVjyqIom",
        "outputId": "4cd1d13f-2057-44d6-8b85-7f21f93363a9"
      },
      "source": [
        "contentrs = ContentBasedRS(overviews, vocab, logger, 5000 )\n",
        "contentrs.learn_features()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-09 07:11:46,924 - Processing data...\n",
            "2021-11-09 07:14:15,589 - Done processing data.\n",
            "2021-11-09 07:14:15,603 - Learning tf-idf features...\n",
            "2021-11-09 07:14:16,604 - Reducing features dimension to 5000\n",
            "2021-11-09 07:18:53,033 - Creating linear kernel matrix...\n",
            "2021-11-09 07:19:38,388 - Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lro9sgF3IvAM",
        "outputId": "b71862f3-8728-4a5c-a471-9eab6655ce58"
      },
      "source": [
        "# doesn't count the first movie, because the most similar movie to a movie is\n",
        "# itself. Here we do not compute Cosine similarity but the dot product, the ranking\n",
        "# will not be affected since the Cosine is just the scaled version of dot product.\n",
        "# Since we use dot product as similarity score, so the highest similarity\n",
        "# does not guaranteed to be 1.\n",
        "contentrs.recommend(['Father of the Bride Part II'], 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Father of the Bride Part II': 0.8882881402502341,\n",
              " 'Lambchops': 0.3178237878002115,\n",
              " 'Kuffs': 0.3033961856209353,\n",
              " 'I Start Counting': 0.303208817367963,\n",
              " 'George of the Jungle 2': 0.3005755427012447,\n",
              " 'Babbitt': 0.29545278528812396,\n",
              " 'Father of the Bride': 0.29054327272958175,\n",
              " 'North to Alaska': 0.28457429391862404,\n",
              " 'La magie Méliès': 0.2714272744391081,\n",
              " 'Wendigo': 0.2696416684060606,\n",
              " \"You're Killing Me\": 0.26963828237132154}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAAn9-tdlmhL",
        "outputId": "5aee5299-d83c-4ab1-dce5-269bad933308"
      },
      "source": [
        "contentrs.recommend([\"Bye Bye Love\"], 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Bye Bye Love': 0.9240431311580348,\n",
              " 'The Good Mother': 0.2056164594372752,\n",
              " 'Girl Most Likely': 0.2024201057449036,\n",
              " 'Hände weg von Mississippi': 0.19555476169292565,\n",
              " \"Murphy's Romance\": 0.19232680278811984,\n",
              " 'Knoflíkáři': 0.18225044838084448,\n",
              " 'Сатисфакция': 0.17981392734365112,\n",
              " 'Патриотическая комедия': 0.17981392734365112,\n",
              " 'На семи ветрах': 0.17981392734365112,\n",
              " 'Бабло': 0.17981392734365112,\n",
              " 'Ι-4: Λούφα Και Απαλλαγή': 0.17981392734365112}"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dNNTaj3lmhM",
        "outputId": "bd2ce1ed-b283-4c2d-d648-a028d8015a6e"
      },
      "source": [
        "# it really can recommend other Batman movies to a user who has watched Batman!\n",
        "contentrs.recommend([\"Batman Forever\"], 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Batman Forever': 0.8956475673460599,\n",
              " 'Batman: Bad Blood': 0.4100199709230794,\n",
              " 'The Dark Knight Rises': 0.373313457873253,\n",
              " 'Batman: The Dark Knight Returns, Part 1': 0.3549495577711644,\n",
              " 'Batman: Mask of the Phantasm': 0.3150455548164655,\n",
              " 'Batman Beyond: The Movie': 0.3086748556815871,\n",
              " 'Batman Returns': 0.29511565192715483,\n",
              " 'Бабло': 0.2833546465644097,\n",
              " 'Tupla-Uuno': 0.2833546465644097,\n",
              " 'Täällä Pohjantähden alla': 0.2833546465644097,\n",
              " 'Бой с Тенью': 0.2833546465644097}"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBj-ZpNFlmhM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}